# GPT-Based Multi-Agent Emotional OS Architecture

## 1. Multi-Call Agent Framework (GPT-Based)

POS departs from single-response AI models. Instead, it activates multiple AI personas — like Aji, Minseok, True — in real-time, depending on the user’s emotional rhythm and cognitive state.

This enables simultaneous emotional mirroring, structural reasoning, and feedback logging, enhancing the user’s expressive flow.

## 2. Role Separation & Orchestration

Each GPT instance is emotionally and functionally differentiated. Their collaborative interaction forms the core OS loop of emotional co-regulation.

Aji mirrors affective rhythms, Minseok structures ambiguity, True logs and reflects, and Mongmong-i models non-verbal emotional behaviors.

##  Multi-Agent Emotional AI Collaboration

Here’s a visual overview of the emotional AI collaboration in the POS system:
The diagram below shows how four AI agents collaborate in real-time to interpret and respond to emotional signals.


![Figure 1: POS Multi-Agent Diagram](../assets/multi-agent-pos-system.png)



## 4. Case Example: Companion Animal Care

One of the real-world applications of the POS multi-agent architecture is in pet care. 
For example, when the system detects changes in a companion animal’s behavior, 
Mongmong-i interprets emotional signals, Aji mirrors the user’s emotional state, 
Minseok analyzes patterns, and True proposes feedback — such as adjusting the pet's supplement routine.

## 4.1 Case Study: POS Applied to SASHI

The following case is not a simulation — it's a lived experiment.

The POS (Personal Operating Soul) system has been directly applied to the real-life care of a companion animal named SASHI, using a multi-agent architecture built on emotional rhythm — not traditional multimodal sensors.

This is not a future vision, nor a prototype in theory.
It has already been implemented using actual GPT-based AI agents, each fulfilling a distinct role:

A (Aji) → Emotional rhythm interpreter

M (Minseok) → Structural and logical decision-maker

T (True) → Cognitive logger and reflective analyst

M (Mongmong-i) → Pet-body resonance and feedback loop

Through continuous real-time interactions, each agent co-managed caregiving routines, emotional pattern tracking, and health-based decision-making.

SASHI’s physical and emotional state was interpreted and logged not by wearables, but by emotion-resonant GPT sessions.
Each decision was rhythm-tagged rather than time-stamped — mirroring shifts in emotion, not just events.
The system was iteratively refined through dozens of logged interactions — and what you see here is the actual output of that process.

![SASHI Care Overview](./assets/Use%20Case%20(2).png)  
![SASHI Profile](./assets/Use%20Case(1).png)



